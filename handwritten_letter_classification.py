# -*- coding: utf-8 -*-
"""Handwritten_Letter_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18F2dUlHqYZlmFBd5HQkyPBQQLdApHrWJ

# PROJECT: HANDWRITTEN LETTER CLASSIFICATION

USING PCA, T-SNE, NEURAL NETWORK, AND SVM

IMPORTING THE DEPENDENCIES
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
import os
import warnings
warnings.filterwarnings('ignore')

"""CREATING THE DATASET"""

path = 'folder/'
files = os.listdir(path)
classes = {"A":0, "B":1, "C":2, "D":3, "E":4, "F":5, "G":6, "H":7, "J":8, "K":9}
print(files)

"""UPLOADING THE DATASET"""

import cv2

X = []
Y = []

for i in classes:
    new_path = path + i
    for img_name in os.listdir(new_path):
        img = cv2.imread(new_path + "/" + img_name)
        X.append(img)
        Y.append(classes[i])
    print(f'Data Uploaded Successfully', {i})

# counting the datapoints for each class label
pd.Series(Y).value_counts()

# Converting the list data type to numpy array to use numpy functions and easy calculations.

print(type(X))
X = np.array(X)
Y = np.array(Y)
print(type(Y))
print(type(X))

# This is basically telling the number of samples in list X
X.shape

# Displaying the 6th image or image at 5th index from a list containing 585 samples.
plt.imshow(X[5], cmap='gray')

"""RESIZING THE DIMENTIONS OF EACH IMAGE IN 28X28 FORM"""

# Resizing the dimention of each image into 28x28 form.
# and then storing these to a new folder named 'resized_data'

X = []
Y = []

resized_images_path = 'resized_data/'

for i in classes:
    new_path = path + i
    for img_name in os.listdir(new_path):
        img = cv2.imread(new_path + '/' + img_name)

        # Resize the image to a specific width and height
        new_width = 28
        new_height = 28
        img = cv2.resize(img, (28,28))

        X.append(img)
        Y.append(classes[i])

        # Save the resized images
        resized_img_path = resized_images_path + img_name
        cv2.imwrite(resized_img_path, img)

    print(f'Data Uploaded Successfully:{i}')

print(img.shape)

# Converting the list to numpy array

print(type(X))
X = np.array(X)
Y = np.array(Y)
print(type(Y))
print(type(X))

print(X.shape)

plt.imshow(X[0],cmap='gray')

"""RESHAPING

The purpose of reshaping X into X_new is to transform the images from their original format into a flattened representation where each sample is represented as a single row. This has been done to feed it into a machine learning algorithm that requires a 2D input format (samples as rows and features as columns)
"""

# Reshaping the Dataset

X_new = X.reshape(len(X),-1)

#Printing the new shape
print(X_new.shape)
print(Y.shape)

"""TRAIN-TEST-SPLIT

TRAINING DATASET --------> 80 Percent

TESTING DATASET ----------> 20 Percent
"""

# Splitting the Dataset into Training Dataset and Test Dataset

X_train, X_test, Y_train, Y_test = train_test_split(X_new, Y, test_size=.20, random_state=10)

# Checking the sample split of training and test data.

print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)

"""468 -----> Train samples

117 -----> Test samples
"""

# Displaying the Flattened Data at index 10.

print(X_train[10])

"""FEATURE SCALING

To Normalizing the pixel values to unit value, We are dividing with the maximum value in the dataset
"""

#Printing the maximum values for train and test data set

print(X_train.max())
print(X_test.max())

# Dividing the train and test data with the maximum value in dataset
X_train = X_train/255
X_test = X_test/255

# Now printing the updated maximum value from the dataset
print(X_train.max())
print(X_test.max())

"""DIMENTIONALITY REDUCTION: USING PRINCIPLE COMPONENT ANALYSIS

FEATURE SELECTION USING PCA
"""

# Printing the shapes of the arrays
print(X_train.shape, X_test.shape)

# To Retain 98% of the variance in the data.
pca = PCA(.98)

# First fits the PCA Model to the Data and then transforms it.
X_train = pca.fit_transform(X_train)

# This ensures that the same transformation  learned from X_train is applied to X_test
X_test = pca.transform(X_test)

# Printing the new shapes of the arrays X_train and X_test after applying PCA.
print(X_train.shape, X_test.shape)

# To print the number of components (n_components) and the number of features (n_features_) in the PCA in the model.
print(pca.n_components)
print(pca.n_features_)

Y_test[:10]

"""MIN MAX SCALING"""

# Min Max Scaling
min_max_scaler = MinMaxScaler()

# Scaling each feature in the range of 0 to 1
X_train_scaled = min_max_scaler.fit_transform(X_train)

# To ensure that the scaling is consistent between the training and testing dataset.
X_test_scaled = min_max_scaler.transform(X_test)

# To print the maximum value among the scaled features
print(X_train_scaled.max())
print(X_test_scaled.max())

# To return the sorted unique elements of an array.
print(np.unique(Y_train))
print(np.unique(Y_test))

print(X_train[10])

print(X_train[10].shape)

"""CONVOLUTIONAL NEURAL NETWORK : CNN

BUILDING A NEURAL NETWORK MODEL
"""

# importing Tensor Flow
import tensorflow as tf

# Setting the random seed to ensure that the results are reproducible
tf.random.set_seed(3)

# Importing Keras Library from Tensor Flow
from tensorflow import keras

#Importing Confusion Matrix to evaluate the performance of classification models.
from tensorflow.math import confusion_matrix

# Defining a Sequential Model with three Dense layers.
model = keras.Sequential([
                          keras.layers.Dense(50, activation='relu'),
                          keras.layers.Dense(50, activation='tanh'),
                          keras.layers.Dense(10, activation='sigmoid')
])

# Compiling the Sequential model using Adam Optimizer
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
             metrics=['accuracy'])

# Here,The entire training dataset is passed through the model 15 times to learn and get trained.
model.fit(X_train, Y_train, epochs=15)

"""ACCURACY ON TEST DATA"""

# Evaluating the trained model using the test data
loss, accuracy = model.evaluate(X_test, Y_test)
print(accuracy)

"""ACCURACY OF CNN ON TRAINING DATA --------> 100 Percent

ACCURACY OF CNN ON TESTING DATA ------> 85 Percent
"""

print(X_test.shape)

# Displaying 6th image in X_test
plt.imshow(X[5])

# Printing the Label associated with the first sample in the test set
print(Y_test[0])

# Using the trained model to make predictions on the test data
Y_predict = model.predict(X_test)

print(Y_predict.shape)

print(Y_predict[0])

# Converting the prediction probabilities to class Label
label_for_first_image = np.argmax(Y_predict[0])
print(label_for_first_image)

# Conerting the prediction probabilities to class label for all test data points
Y_predict_labels = [np.argmax(i) for i in Y_predict]
print(Y_predict_labels)

print(Y_predict)

"""Y_test ----------> True Labels

Y_predict_labels ---------> Predicted Labels

CONFUSION MATRIX
"""

# Building a confusion matrix using true labels and predicted labels.
conf_matrix = confusion_matrix(Y_test, Y_predict_labels)

# Printing the Confusion Matrix
print(conf_matrix)

"""VISUALIZING THE CONFUSION MATRIX"""

# Heatmap visualization of Confusion Matrix
plt.figure(figsize=(15,7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.ylabel('True Labels')
plt.xlabel('Predicted Labels')

"""HYPER PARAMETER TUNING"""

!pip install keras-tuner

# Hyper Parameter Tuning
from kerastuner.tuners import RandomSearch
from kerastuner.engine.hyperparameters import HyperParameters

# Define the model builder function
def build_model(hp):
    model = keras.Sequential()
    model.add(keras.layers.Dense(units=hp.Int('units_1', min_value=32, max_value=128, step=32),
                                 activation=hp.Choice('activation_1', values=['relu', 'tanh', 'sigmoid'])))
    model.add(keras.layers.Dense(units=hp.Int('units_2', min_value=32, max_value=128, step=32),
                                 activation=hp.Choice('activation_2', values=['relu', 'tanh', 'sigmoid'])))
    model.add(keras.layers.Dense(units=hp.Int('units_3', min_value=32, max_value=128, step=32),
                                 activation=hp.Choice('activation_3', values=['relu', 'tanh', 'sigmoid'])))
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model

# Instantiate the tuner
tuner = RandomSearch(build_model, objective='val_accuracy', max_trials=10,
                    executions_per_trial=1, directory='tuner_results',
                    project_name='my_model_tuning')

# Perform Hyperparameter Search
tuner.search(X_train, Y_train, epochs=10, validation_data=(X_test, Y_test))

# Retrieve the best model
best_model = tuner.get_best_models(1)[0]

# Fit the best model with the complete training data
best_model.fit(X_train, Y_train, epochs=10)

# Evaluate the best model on the test data
test_loss, test_accuracy = best_model.evaluate(X_test, Y_test)
train_loss, train_accuracy = best_model.evaluate(X_train, Y_train)

# Printing the Best Parameters
best_hyperparameters = tuner.get_best_hyperparameters(1)[0]

print("Best Hyperparameters: ", best_hyperparameters)

# Printing the best parameters
best_hyperparameers = tuner.get_best_hyperparameters(1)[0]
print("Best Hyperparameters: ", best_hyperparameters)

# Printing the Accuracy of Training and Test Dataset
print(f'Accuracy on Training Data: ', train_accuracy)
print(f'Accuracy on Test Data: ', test_accuracy)

"""Training Accuracy --------> 100 Percent

Testing Accuracy ----------> 88 Percent

DIMENTIONALITY REDUCTION TECHNIQUE : T-SNE

IMPLEMENTING T-SNE
"""

# Reducing the dimentions of the dataset to visualize t-SNE
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_train)

"""VISUALIZING THROUGH T-SNE"""

# Visualizing the Training Dataset through t-SNE
import matplotlib.pyplot as plt

plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=Y_train)
plt.title("t-SNE Visualization")
plt.xlabel("Dimention 1")
plt.ylabel("Dimention 2")
plt.show()

"""SUPPORT VECTOR MACHINE : SVM

BUILDING SVM MODEL
"""

# Building SVM Model
from sklearn import svm
classifier = svm.SVC(kernel='linear')

# Fitting the SVM Model to Training Dataset

classifier.fit(X_train, Y_train)

# Accuracy score on training data

X_train_prediction = classifier.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

# Printing the Accuracy of Training Dataset

print("Accuray score of the training data: ", training_data_accuracy)

# Accuracy score on test data

X_test_prediction = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

# Printing the Accuracy of Test Data

print("Accuracy Score of the Test Data: ", test_data_accuracy)

"""TRAINING ACCURACY --------> 100 Percent

TESTING ACCURACY  --------> 94 Percent

HYPER-PARAMETER TUNING OF SVM MODEL
"""

# Hyper Parameter Tuning
param_grid = {'C' : [0.1, 1, 10], 'gamma': [0.1, 1, 'scale', 'auto']}

grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, Y_train)


# Retrieve the best hyper-parameters and model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Fit the best model on Training Data
best_model.fit(X_train, Y_train)

# Accuracy score on Training Data
training_data_accuracy = best_model.score(X_train, Y_train)

# Accuracy score on Test Data
test_data_accuracy = best_model.score(X_test, Y_test)

# Print the Best Parameters and Accuracy
print("Best Hyperparameters: ",best_params )
print("Traning Accuracy: ", training_data_accuracy)
print("Test Accuracy: ", test_data_accuracy)

"""TRAINING ACCURACY OF SVM MODEL : 100 Percent

TEST ACCURACY OF SVM MODEL : 94 Percent

BEST HYPE-PARAMETERS : {'C':0.1, 'gamma': 0.1}

"""